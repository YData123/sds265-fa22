{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ad167d",
   "metadata": {},
   "source": [
    "## Introductory Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 5 is due Thursday, December 1 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/9209/discussion/). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Bayesian inference\n",
    "2. Topic models\n",
    "\n",
    "The first two problems test some of the basics of Bayesian inference. The third problem has you building topic models and using them to fit some linear regressions. The fourth problem asks you to build topic models on the UN data.\n",
    "\n",
    "Note: The assignment looks longer than it really is. We step you through most of the code that you need. But it's still on the long side. Although the assignment is due in three weeks, we encourage you to start early!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210037fd",
   "metadata": {},
   "source": [
    "### Problem 1: Let the good times roll (10 points)\n",
    "\n",
    "Consider the scenario of rolling a 4-sided die with the numbers $1$, $2$, $3$, and $4$ on its faces. Suppose we roll this die many times and get a collection of $n$ outcomes represented by $X_{1}, X_{2}, ..., X_{n}$. Here each $X_{i}$ is a random variable that independently follows a Multinomial$(p_{1}, p_{2}, p_{3}, p_{4})$ model (where $p_{1}+p_{2}+p_{3}+p_{4}=1$).\n",
    "\n",
    "This die may or may not be fair. If it were fair then $p_{1}=p_{2}=p_{3}=p_{4}=0.25$, but since we are uncertain about these parameters we treat them as random and the problem requires Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096059b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:45.484822Z",
     "start_time": "2021-11-12T15:01:45.484807Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3db62",
   "metadata": {},
   "source": [
    "#### Part (a)\n",
    "\n",
    "For (a) we will assume that $(p_{1}, p_{2}, p_{3}, p_{4})$ follows a Dirichlet$(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}, \\alpha_{4})$ distribution where $\\alpha_{1}, \\alpha_{2}, \\alpha_{3}, \\alpha_{4}$ are unknown, positive-valued parameters. Suppose we have a prior belief that the four-sided die is close to being fair. This is represented by $\\alpha_{1}= \\alpha_{2}= \\alpha_{3}= \\alpha_{4} = c$ for some positive real number $c$.\n",
    "\n",
    "For $c = 0.1, 1, 10, 30, 60, 100, 1000$ draw $1000$ samples of $(p_{1}, p_{2}, p_{3}, p_{4})$ from a Dirichlet$(c,c,c,c)$ distribution. For this sample, calculate the mean and standard deviation of $p_{1}$. Create a plot of $log(c)$ vs. the mean and another plot of $log(c)$ vs. the standard deviation. Describe in your own words what happens to these two quantities as $c$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a786ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:45.492753Z",
     "start_time": "2021-11-12T15:01:45.490009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07692f0d",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cdb2a",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "\n",
    "The following cell loads 10,000 rolls for the four-sided die. $[1,0,0,0]$ indicates that the die landed on face $1$, $[0,1,0,0]$ indicates that the die landed on face $2$, and so on. For $c = 0.1, 1, 10, 30, 60, 100, 1000$, use Dirichlet$(c,c,c,c)$ as the prior distribution for $(p_{1}, p_{2}, p_{3}, p_{4})$. *Using only the first $100$ rolls of the die*, calculate the mean of the posterior distribution. What do you notice about the posterior mean as $c$ increases?\n",
    "\n",
    "Give code to compute the answer and plot the results. Also, give a markdown cell with a mathematical expression for the solution.\n",
    "\n",
    "Hint: The mean of the Dirichlet$(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}, \\alpha_{4})$ is $\\left( \\dfrac{\\alpha_{1}}{\\alpha_{1} + \\alpha_{2} +\\alpha_{3} +\\alpha_{4}}, \\dfrac{\\alpha_{2}}{\\alpha_{1} + \\alpha_{2} +\\alpha_{3} +\\alpha_{4}}, \\dfrac{\\alpha_{3}}{\\alpha_{1} + \\alpha_{2} +\\alpha_{3} +\\alpha_{4}}, \\dfrac{\\alpha_{4}}{\\alpha_{1} + \\alpha_{2} +\\alpha_{3} +\\alpha_{4}} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb73fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:45.505694Z",
     "start_time": "2021-11-12T15:01:45.500072Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/X.pkl')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c8170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:45.507087Z",
     "start_time": "2021-11-12T15:01:45.507075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7f262",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a414fb",
   "metadata": {},
   "source": [
    "#### Part (c)\n",
    "\n",
    "Now repeat the process in Part (b), but with sample sizes $N = 100, 200, 300, ..., 9900, 10000$. For each value of $c$, create a plot that shows the trend of the posterior mean for $p_{1}$ as a function of sample size $N$. Create a similar plot for $p_{2}$, $p_{3}$, and $p_{4}$. Explain what these plots illustrate about the choice of prior and the sample size. What do you estimate were the true parameters used to generate this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c62f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:45.516725Z",
     "start_time": "2021-11-12T15:01:45.514699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff4600",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0c725",
   "metadata": {},
   "source": [
    "### Problem 2: Toy Story (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db81e9a",
   "metadata": {},
   "source": [
    "Gibbs sampling is one of the commonly used approach to approximate the inference for Latent Dirichlet Allocation model. In this problem, we will use the toy example from class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614a475",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/diagram.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578893fe",
   "metadata": {},
   "source": [
    "Assume that there are 3 documents and 15 words in the corpus. We would like to build a topic model with 3 topics. The proportions parameter is $\\alpha$ and the topic parameter is $\\eta$. The table below shows an assignment of topics to words in the toy corpus at one stage of the Gibbs sampling algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cf434",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/words.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee261b92",
   "metadata": {},
   "source": [
    "Using only these assignment id $Z$ for each word, the following problems ask you \n",
    "to calculate the posterior topic proportions for each document, and word probabilities \n",
    "for one word in each of the three topics. To answer these questions you only need\n",
    "to use the basic properties of the Dirichlet distribution as a prior for \n",
    "a multinomial, as presented in class (and in the notes on Bayesian inference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76d830",
   "metadata": {},
   "source": [
    "#### Problem 2.1: Per-document topic proportions\n",
    "\n",
    "Given the $Z$ values in the table, what are the posterior distributions of $\\theta_{d}$ for documents $D_{1}$, $D_{2}$ and $D_{3}$ from left to right. Assume the prior over $\\theta$ is \n",
    "$\\mbox{Dirichlet}(\\alpha, \\alpha, \\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa7474",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8883773",
   "metadata": {},
   "source": [
    "#### Problem 2.2: Topics\n",
    "\n",
    "Here are the 15 words in our corpus:\n",
    "\n",
    "addiction, brother, baseball, catcher, daughter, divorce, drug, hit, inning, illegal, meth, mother, swing, son, steroids\n",
    "\n",
    "What is the posterior mean for the probability $p(\\mbox{addiction} | \\mbox{topic 1})$? \n",
    "Assume that the prior distribution over the topics is $\\mbox{Dirichlet}(\\eta,...\\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec1c36",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b3d79",
   "metadata": {},
   "source": [
    "What is the posterior mean of the probability $p(\\mbox{baseball}| \\mbox{topic 2})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f69707",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f28ca",
   "metadata": {},
   "source": [
    "What is the posterior mean of the probability $p(\\mbox{divorce} | \\mbox{topic 3})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4dc94",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6b751",
   "metadata": {},
   "source": [
    "## Problem 3: Read before you buy! (30 points)\n",
    "\n",
    "![zillow](https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/zillow.png)\n",
    "\n",
    "### Overview of the problem\n",
    "\n",
    "Here we have a dataset of single family houses sold in Connecticut near the beginning of 2021, collected from [Zillow](https://www.zillow.com/homes/connecticut_rb/). You will build linear models of the price for which each house sold, based on its characteristics given in the real estate listing. Such characteristics include internal square footage, the year it was built, the bedroom count, the bathroom count, and the area of the lot. \n",
    "\n",
    "But there is also usually a lengthy description written by the real estate agent. Is there any additional information hidden in this description that would help improve the model of the price? This is the question we focus on in this problem.\n",
    "\n",
    "Answering such a question is difficult because the description is written in natural language with thousands of different words. Here we use topic models as a dimension reduction technique. Specifically, instead of using thousands of possible words, and how many times they show up in each house description, we reduce the words to the topic proportions $\\theta_d$ for each document, obtained by posterior inference. These proportions are combined with the other quantitative variables in a linear model with the logarithm of the house price as the response variable. \n",
    "\n",
    "*Important note:* At first glance, this problem looks really long. But this is deceiving. \n",
    "After reading in the data, we have you make some plots of the log-transformed variables. \n",
    "After that, you just need to run the code that leads up to training a 10-topic topic model, \n",
    "and fitting a linear model using the resulting topic proportions. After this, you are asked to compare the results to those obtained with a 3-topic model. To do this, you can simply copy the code used for the 10-topic model. After that, the crux of the problem is to analyze, understand, and describe the results.\n",
    "\n",
    "Acknowledgment: The data were scraped and the analysis was done by [Parker Holzer](https://parkerholzer.github.io/), as he began his search for a new house for his family after beginning a job as a data scientist. Thanks Parker!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977a84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.152805Z",
     "start_time": "2021-11-12T15:01:45.572491Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b3c1a",
   "metadata": {},
   "source": [
    "### Read in and clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a5007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.594652Z",
     "start_time": "2021-11-12T15:01:47.154274Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct_homes = pd.read_csv('https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/ct_zillow.csv')\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551b7b6",
   "metadata": {},
   "source": [
    "#### Transform the data\n",
    "\n",
    "We add columns to `ct_homes` called `logAREA`, `logLOTSIZE`, and `logPRICE` that take the logarithms of the corresponding columns in the original data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea5a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.614588Z",
     "start_time": "2021-11-12T15:01:47.596230Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_homes['logAREA'] = np.log(ct_homes['AREA'])\n",
    "ct_homes['logLOTSIZE'] = np.log(ct_homes['LOTSIZE'])\n",
    "ct_homes['logPRICE'] = np.log(ct_homes['PRICE'])\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e3c1a",
   "metadata": {},
   "source": [
    "#### 3.1 Plot the data \n",
    "\n",
    "1. Show histograms of each of the log-transformed columns.\n",
    "\n",
    "1. Our regression models will use these transformed values. Why might it be preferable to use the logarithms rather than the original data? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3161f46",
   "metadata": {},
   "source": [
    "[Your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40048f",
   "metadata": {},
   "source": [
    "Let's look at one of the descriptions as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da61ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.620301Z",
     "start_time": "2021-11-12T15:01:47.617177Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 9\n",
    "ct_homes[\"DESCRIPTION\"][example]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab71c09",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "\n",
    "The following two functions will be used to clean up the text a bit and separate into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03396e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.624584Z",
     "start_time": "2021-11-12T15:01:47.621530Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_description(desc):\n",
    "    if type(desc) == float:\n",
    "        desc = \"\"\n",
    "    words = [re.sub(r'[^a-z]', '', w) for w in desc.lower().split(' ')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def reduce_to_vocabulary(desc, vocab):\n",
    "    return ' '.join([w for w in cleanup_description(desc).split(' ') if w in vocab])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8b94e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.629948Z",
     "start_time": "2021-11-12T15:01:47.625889Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_description(ct_homes['DESCRIPTION'][example])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5709227",
   "metadata": {},
   "source": [
    "\n",
    "#### Next we build a vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f261dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.919063Z",
     "start_time": "2021-11-12T15:01:47.631311Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    vocab.update(cleanup_description(dsc).split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b617ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.923544Z",
     "start_time": "2021-11-12T15:01:47.920814Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f511e2",
   "metadata": {},
   "source": [
    "#### Remove words that are either too common or too rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4d987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.081138Z",
     "start_time": "2021-11-12T15:01:47.925007Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter(token for token in vocab.elements() if vocab[token] > 5)\n",
    "stop_words = [item[0] for item in vocab.most_common(50)]\n",
    "vocab = Counter(token for token in vocab.elements() if token not in stop_words)\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aaca31",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd2910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.088577Z",
     "start_time": "2021-11-12T15:01:48.084325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc = ct_homes['DESCRIPTION'][example]\n",
    "print('Original description:\\n---------------------')\n",
    "print(desc)\n",
    "\n",
    "print('\\nCleaned up text:\\n----------------')\n",
    "print(cleanup_description(desc))\n",
    "\n",
    "print('\\nReduced to vocabulary:\\n----------------------')\n",
    "print(reduce_to_vocabulary(desc, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e69cef",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751f964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.097985Z",
     "start_time": "2021-11-12T15:01:48.091685Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = {idx: pair[0] for idx, pair in enumerate(vocab.items())}\n",
    "word2id = {pair[0]: idx for idx, pair in enumerate(vocab.items())}\n",
    "\n",
    "s = 'nyc'\n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for '%s': %d\" % (s,word2id[s]))\n",
    "print(\"Word for identifier %d: %s\" % (word2id[s], id2word[word2id[s]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c7d5b",
   "metadata": {},
   "source": [
    "#### Map to word id format\n",
    "\n",
    "Now, use the format required to build a language model, mapping each word to its id, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c7296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.656600Z",
     "start_time": "2021-11-12T15:01:48.101461Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "    toks = clean.split(' ')\n",
    "    tokens.append(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b84c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.724420Z",
     "start_time": "2021-11-12T15:01:48.658177Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for toks in tokens:\n",
    "    tkn_count = Counter(toks)\n",
    "    corpus.append([(word2id[item[0]], item[1]) for item in tkn_count.items()])\n",
    "    \n",
    "dsc = ct_homes['DESCRIPTION'][example]\n",
    "clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "toks = clean.split(' ')\n",
    "print(\"Abstract, tokenized:\\n\", toks, \"\\n\")\n",
    "print(\"Abstract, in corpus format:\\n\", corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84808f81",
   "metadata": {},
   "source": [
    "#### Build a Topic Model with 10 topics\n",
    "\n",
    "Note: Don't worry about the various settings used in the call to `LdaModel`. If you want to read up on these, just check out the documentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62cbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.387268Z",
     "start_time": "2021-11-12T15:01:48.725885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tm = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=id2word,\n",
    "                                     num_topics=10, \n",
    "                                     random_state=100,\n",
    "                                     chunksize=100,\n",
    "                                     passes=10,\n",
    "                                     alpha='auto',\n",
    "                                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605f96a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.405918Z",
     "start_time": "2021-11-12T15:01:53.388536Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = pd.DataFrame({'word rank': np.arange(1,num_words+1)})\n",
    "for k in np.arange(num_topics): \n",
    "    topic = tm.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words['topic %d' % k] = words\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f927c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.415623Z",
     "start_time": "2021-11-12T15:01:53.407114Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dist = tm.get_document_topics(corpus[example])\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = pd.DataFrame()\n",
    "topic_dist_table['Topic'] = topics\n",
    "topic_dist_table['Probabilities'] = probabilities\n",
    "topic_dist_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab49c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.704131Z",
     "start_time": "2021-11-12T15:01:53.417119Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table['Topic'], topic_dist_table['Probabilities'], align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5c942",
   "metadata": {},
   "source": [
    "### Include the topic proportions $\\theta_d$ for each house \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25f3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.326782Z",
     "start_time": "2021-11-12T15:01:53.705671Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "theta = pd.DataFrame({\"Theta0\": np.zeros(ct_homes.shape[0])})\n",
    "for t in np.arange(1,num_topics):\n",
    "    theta[\"Theta\"+str(t)] = np.zeros(ct_homes.shape[0])\n",
    "    \n",
    "for i in np.arange(ct_homes.shape[0]):\n",
    "    for t in tm.get_document_topics(corpus[i]):\n",
    "        theta.loc[i,\"Theta\"+str(t[0])] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfba96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.352081Z",
     "start_time": "2021-11-12T15:01:56.328006Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_topics = ct_homes.join(theta)\n",
    "ct_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fcbb6",
   "metadata": {},
   "source": [
    "#### Fit a linear model with the topic proportions included\n",
    "\n",
    "We now fit a linear model with the topic proportions included. Note that \n",
    "the proportions satisfy $\\theta_0+\\theta_1+\\cdots + \\theta_9 = 1$. Therefore, we remove one of them, since it is redundant. If we don't do this the linear model will be harder to interpret!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae887f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.398437Z",
     "start_time": "2021-11-12T15:01:56.353552Z"
    }
   },
   "outputs": [],
   "source": [
    "model = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT + Theta0 + \" +\n",
    "               \"Theta1 + Theta2 + Theta3 + Theta4 + Theta5 + Theta6 + Theta7 + Theta8\", data=ct_topics).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef235600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.604079Z",
     "start_time": "2021-11-12T15:01:56.399978Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(model.resid, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fc760",
   "metadata": {},
   "source": [
    "### Model without the topics included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab0c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.627406Z",
     "start_time": "2021-11-12T15:01:56.605673Z"
    }
   },
   "outputs": [],
   "source": [
    "model_without_topics = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT\", data=ct_topics).fit()\n",
    "model_without_topics.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b3fba",
   "metadata": {},
   "source": [
    "#### 3.2 Plot the residuals\n",
    "\n",
    "On a single plot, show a histogram of the residuals of the model without the topics, \n",
    "and the residuals of the model with the topics. Give a legend that shows which is which.\n",
    "Comment on the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21244fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.630729Z",
     "start_time": "2021-11-12T15:01:56.628696Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e7c8f",
   "metadata": {},
   "source": [
    "#### 3.3 Quantify the improvement: R-squared\n",
    "\n",
    "How do the two models compare in terms of R-squared? What do these numbers mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af17b20",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b874435",
   "metadata": {},
   "source": [
    "#### 3.4 Quantify the improvement: MSE decrease\n",
    "\n",
    "What is the percent decrease in the mean-squared-error of the model with the topics\n",
    "compared to the model that ignores the descriptions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0a06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.634159Z",
     "start_time": "2021-11-12T15:01:56.632262Z"
    }
   },
   "outputs": [],
   "source": [
    "# [your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24064c52",
   "metadata": {},
   "source": [
    "#### 3.5 Quantify the improvement: LOOCV\n",
    "\n",
    "What is the percent decrease in the leave-one-out-cross-validation (LOOCV) error?\n",
    "Recall from class that the following formula can be used to calculate this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/loocv.png\" width=\"410\" align=\"center\">\n",
    "\n",
    "<br>\n",
    "\n",
    "The following line of code computes this for one of the models:\n",
    "\n",
    "`np.mean((model.resid/(1 - model.get_influence().hat_matrix_diag))**2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8479f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.637660Z",
     "start_time": "2021-11-12T15:01:56.635497Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86f4d9",
   "metadata": {},
   "source": [
    "#### 3.6 Repeat for three topics \n",
    "\n",
    "Now, repeat the above steps for a topic model that is trained using only three (3) topics. Specifically:\n",
    "\n",
    "1. Train a model with three topics\n",
    "1. Display the top words in each of the three topics\n",
    "1. Augment the `ct_homes` data with the resulting topic proportions $\\theta$\n",
    "1. Fit a linear model *using only the first two of the three* proportions\n",
    "1. Plot a histogram of the residuals of the three linear models together\n",
    "1. Comment on the improvement over the baseline in terms of R-squared, MSE, and LOOCV compared with the previous two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf710b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.641409Z",
     "start_time": "2021-11-12T15:01:56.639322Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5af21",
   "metadata": {},
   "source": [
    "#### 3.7 Interpretation\n",
    "\n",
    "Now, interpret the model. Use the coefficients of the linear model to \n",
    "help interpret the meaning of the topics. Comment on what this says \n",
    "about the effectiveness of the topic model for predicting the sale price \n",
    "of the house. Does it make intuitive sense? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaaf5eb",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:iML]",
   "language": "python",
   "name": "conda-env-iML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
