{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Machine Learning: Assignment 4\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 4 is due Thursday, November 10 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/9209/discussion/). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Language models\n",
    "2. Work embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Gutenberg Books Language Models (15 points)\n",
    "\n",
    "For this problem you will process books from the [Project Gutenberg](https://www.gutenberg.org/) site which is a public respository of large numbers of books that are in the public domain. You'll build *character-based* (as opposed to word-based) language models on one book, and predict the letters of the other book using the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function `read_url` reads in the text at the given url, and then uses some \n",
    "[regular expressions](https://www.w3schools.com/python/python_regex.asp) to process the book, removing \n",
    "everything but the letters a-z, space and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "import re\n",
    "def read_url(url): \n",
    "    return re.sub('\\\\s+', ' ', urlopen(url).read().decode())\n",
    "\n",
    "def process_text(text):\n",
    "    text = re.sub('[^a-zA-z .]', '', text.lower())\n",
    "    return re.sub('[\\[\\]\\_]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gutenberg.org/cache/epub/76/pg76.cover.medium.jpg\" width=\"110\" align=\"top\">\n",
    "\n",
    "The online book for \"Adventures of Huckleberry Finn,\" by Mark Twain, is [here](https://www.gutenberg.org/ebooks/76).\n",
    "From this web site you can see various metadata for the book as well as the [link the text itself](https://www.gutenberg.org/files/76/76-0.txt), which is [https://www.gutenberg.org/files/76/76-0.txt](https://www.gutenberg.org/files/76/76-0.txt)\n",
    "\n",
    "The book for Mark Twain's \"A Connecticut Yankee in King Arthur's Court\" is [here](https://www.gutenberg.org/ebooks/86).\n",
    "In the following cell we read in both of these books, and remove all characters except a-z, space, and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huck_finn_url = 'https://www.gutenberg.org/files/76/76-0.txt'\n",
    "huck_finn_text_raw = read_url(huck_finn_url)\n",
    "huck_finn_text = process_text(huck_finn_text_raw)\n",
    "\n",
    "ct_yankee_url = 'https://www.gutenberg.org/files/86/86-0.txt'\n",
    "ct_yankee_text_raw = read_url(ct_yankee_url)\n",
    "ct_yankee_text = process_text(ct_yankee_text_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of raw text:\\n\")\n",
    "print(huck_finn_text_raw[10000:11000])\n",
    "\n",
    "print(\"\\nSample of processed text:\\n\")\n",
    "print(huck_finn_text[10000:11000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gutenberg.org/cache/epub/1342/pg1342.cover.medium.jpg\" width=\"110\" align=\"top\">\n",
    "\n",
    "\n",
    "The online book for \"Pride and Prejudice\", by Jane Austen, is [here](https://www.gutenberg.org/ebooks/1342).\n",
    "And [here](https://www.gutenberg.org/ebooks/158) is the online book for Jane Austen's \"Emma\".  In the following cell we read in both of these books, and remove all characters except a-z, space, and period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_prejudice_url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "pride_and_prejudice_text_raw = read_url(pride_and_prejudice_url)\n",
    "pride_and_prejudice_text = process_text(pride_and_prejudice_text_raw)\n",
    "\n",
    "\n",
    "emma_url = 'https://www.gutenberg.org/files/158/158-0.txt'\n",
    "emma_text_raw = read_url(emma_url)\n",
    "emma_text = process_text(emma_text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of raw text:\\n\")\n",
    "print(emma_text_raw[10000:11000])\n",
    "\n",
    "print(\"\\nSample of processed text:\\n\")\n",
    "print(emma_text[10000:11000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines some helper code. You should just run this cell; do not change any of the code. \n",
    "\n",
    "The first function, `ngrams`, takes some input text and a value of `n`. The function then \n",
    "iterates over the string and counts the number of occurrences of each substring of `n` characters. This is done with the very handy `Counter` class. \n",
    "\n",
    "We then define a class `language_model` that is a 4-gram character-based language model. The probability of the \"next character\" is computed using linear interpolation, as described in class.  A weight is assigned to unigrams, bigrams, trigrams, and four-grams (quadgrams?). The bigram probability that, for example, the letter `t` follows the letter `h` is the count of the bigram `ht` divided by the count of the unigram `h`. We add a little bit (1e-10) to the denominator to avoid dividing by zero. \n",
    "\n",
    "We return the logarithm of the probability, because this will be convenient when computing perplexities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, n=2):\n",
    "    return Counter([text[(i-n):i] for i in np.arange(n, len(text)+1)])\n",
    "\n",
    "class language_model:\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.one = ngrams(text, 1)\n",
    "        self.two = ngrams(text, 2)\n",
    "        self.three = ngrams(text, 3)\n",
    "        self.four = ngrams(text, 4)\n",
    "        self.weight = [0.1, 0.2, 0.3, 0.4]\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weight = weights / np.sum(weights)\n",
    "        \n",
    "    def log_probability(self, gram):\n",
    "        numer = [self.one[gram[3:]], self.two[gram[2:]], self.three[gram[1:]], self.four[gram[0:]]]\n",
    "        denom = [sum(self.one[g] for g in self.one), self.one[gram[2:3]], self.two[gram[1:3]], self.three[gram[0:3]]]\n",
    "        prob = 0\n",
    "        for i in np.arange(4):\n",
    "            prob += self.weight[i] * numer[i] / (denom[i]+1e-10)\n",
    "        return np.log(prob)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1\n",
    "\n",
    "Just to be sure we understand what a character-based language model is, let's write an expression \n",
    "for the probability in an example. Suppose the language model assigns \n",
    "weight $w_1 = 0.1$ to the unigram model, weight $w_2 = 0.2$ to the bigram model, weight $w_3 = 0.3$ to the trigram model, and weight $w_4 = .4$ to the four-gram model. Note that we must have $w_1+w_2+w_3+w_4 = 1$.\n",
    "\n",
    "Write an expression for the probability $p(\\mbox{z} \\,|\\, \\mbox{qui})$ that the letter $\\mbox{z}$ follows the three letters $\\mbox{qui}$. Assume that the unigram, bigram, trigram, and four-gram components are given by ratios of \n",
    "counts in the training data, as in the code above. For example, the bigram probability would be written as \n",
    "\n",
    "$$ \\frac{\\mbox{count}(iz)}{\\mbox{count}(i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your solution here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the cell below constructs two language models, one on the text of Jane Austen's \"Emma\", the other on \n",
    "the text of Mark Twain's \"Huckleberry Finn\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_lm = language_model(emma_text)\n",
    "huck_finn_lm = language_model(huck_finn_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2\n",
    "\n",
    "In this sub-problem, your job is to write a function that takes a language model `lm`, and a text string `text`, and computes the perplexity of the language model on the text. \n",
    "\n",
    "Hints:\n",
    "* Your function can ignore the first three characters of the text. Thus, you can begin predicting the fourth character from the first three.\n",
    "* Either extract the sequence of 4-character substrings, or make a call to `ngrams(text, n=4)` to get a set of 4-grams and their counts on the text.\n",
    "* Compute the logarithm of probability of the text. If you compute the probability, you will get a very tiny number and numerical \"underflow\". \n",
    "* Use the function `lm.log_probability` where `lm` is a instance of the class `language_model`. For example, `emma_lm.log_probability('emma')` will compute the logarithm of the probability that the character \"a\" follows the three characters \"emm\" using the language model computed on Jane Austen's \"Emma\". \n",
    "* Once you have the logarithm of the probability of the entire text, you'll need to scale appropriately and then take the exponential, using `np.exp`.\n",
    "* Work out the formula by \"pencil and paper\" before trying to write the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text, lm):\n",
    "    # your code here\n",
    "    return 1 # replace with appropriate value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3 \n",
    "\n",
    "To test your implementation of the perplexity function, evaluate the followign cell. This \n",
    "computes the perplexity of the \"Emma\" language model on all four of the books: \"Emma\", \"Pride and Prejudice\", \"Huckleberry Finn\", and \"Connecticut Yankee\". For this problem, you will be graded on whether or not you get the correct four numbers for each of these perplexities.\n",
    "\n",
    "Just run the following cell, which will evaluate the perplexities and print them out. No need to modify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_perp = compute_perplexity(huck_finn_text, emma_lm)\n",
    "ct_perp = compute_perplexity(ct_yankee_text, emma_lm)\n",
    "pp_perp = compute_perplexity(pride_and_prejudice_text, emma_lm)\n",
    "em_perp = compute_perplexity(emma_text, emma_lm)\n",
    "\n",
    "print(\"Perplexity on Huckleberry Finn: %.2f\" % hf_perp)\n",
    "print(\"Perplexity on Connecticut Yankee: %.2f\" % ct_perp)\n",
    "print(\"Perplexity on Pride and Prejudice: %.2f\" % pp_perp)\n",
    "print(\"Perplexity on Emma: %.2f\" % em_perp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4 \n",
    "\n",
    "Now, interpret your results above. Explain the meaning of perplexity for a character-based language model. Which book has the lowest perpexity? Why is this? Which book has the second smallest perplexity? Does this make sense? Explain. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.5\n",
    "\n",
    "Next, mix it up by computing the perplexity of the \"Huckleberry Finn\" language model on each of the four books. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your code and Markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.6\n",
    "\n",
    "Finally, in this problem you should explore the choice of the weights assigned to unigrams, bigrams, trigrams, and four-grams. Recall that to set the weights on the language model `lm` you can use a function call like\n",
    "`lm.set_weights([.25, .25, .25, .25])`\n",
    "\n",
    "1. Try to find weights for the \"Emma\" model so that the perplexity of \"Pride and Prejudice\" is as small as possible. What weights do you find? Do these weights make sense to you?\n",
    "\n",
    "\n",
    "2. Try to find weights for the \"Emma\" model so that the perplexity of \"Huckleberry Finn\" is as small as possible. What happens to the perplexity for \"Pride and Prejudice\"? Does this perplexity exceed that of \"Huck Finn\"? How do the weights you find compare to those you found above? Can you explain intuitively why they are different?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your code and markdown here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.7 (Extra credit: 2 points)\n",
    "\n",
    "Choose two books from the Gutenberg collection that are by the same author (different from Twain and Austen). Build a language model on one of the books, and test, by evaluating perplexity, on all of the other books, five books in all.  Where does the second book of the new author rank? Do the result make sense? Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Word embedding experiments (15 points)\n",
    "\n",
    "In this problem you will run experiments on word embeddings using two different algorithms or configurations: (1) word2vec embeddings trained on the \"text8\" Wikipedia corpus (2) GloVe embeddings pre-trained on a much larger corpus.\n",
    "\n",
    "The text8 data are described here: http://mattmahoney.net/dc/textdata.html. The text8 file is a 100MB excerpt of Wikipedia. This small dataset is sufficient for our exploratory purposes, but note that it is far too small for any real application.\n",
    "In the next few parts of this problem, you will construct word embeddings from the Wikipedia data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2Vec` is a popular word embedding method. The following code will construct 100 dimensional embeddings on the text8 data.\n",
    "\n",
    "Note: If you got the error message \"unexpected keyword argument 'size'\", it is because you are using a later version of gensim. You can solve the problem by using vector_size instead of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus('https://raw.githubusercontent.com/YData123/sds265-fa22/master/assignments/assn4/text8')\n",
    "model = word2vec.Word2Vec(sentences, size=100, window=10, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `model.wv`, keyed by words (as strings), has values which are the word embeddings (as numpy arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also work with pre-trained GloVe embeddings. These embeddings were trained on a large corpus containing 6 billion tokens. You can load these embedding vectors using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gdl\n",
    "glove = gdl.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample evaluation: puppy is to dog as what is to kitten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive = ['dog', 'kitten'], negative = ['puppy'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Experiments\n",
    "\n",
    "Conduct the following experiments with **both sets of embeddings**: the local word2vec embeddings, and the pre-trained GloVe embeddings. Based on the available memory on your computer, you may need to perform the experiments for each set of embeddings in a fresh Python session. Comment on the qualitative differences in the results for each of the embeddings.\n",
    "\n",
    "\n",
    "#### 2.1 Find the closest words\n",
    "\n",
    "For each of the following words, find the 5 closest words in the embedding space, and report your results:\n",
    "\n",
    "          yale, physics, dessert, einstein, algebra, fish\n",
    "          \n",
    "Here, \"closest\" means in terms of cosine similarity. See the gensim documentation; you might want to use the most similar function, or a related function. Choose five other query words yourself, and for each of them show the closest words in the embedding space. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Complete analogies\n",
    "\n",
    "A surprising consequence of some word embedding methods is that they can be used to resolve analogies, like\n",
    "\n",
    "                   france :  paris ::  england :  ?\n",
    "                   \n",
    "You can \"solve\" this analogy by computing the nearest embedding vector to $v$ where, $v = v_{paris} − v_{france} + v_{england}$.\n",
    "\n",
    "Solve the following analogies with both sets of word embeddings and report your results:\n",
    "\n",
    "                   france :  paris ::  england :  ?\n",
    "                   france :  paris ::  germany :  ?\n",
    "                     queen :  woman ::  king :  ?\n",
    "                     \n",
    "Choose five other analogies yourself, and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualize embeddings\n",
    "\n",
    "Use the t-SNE dimensionality reduction technique to visualize the embeddings in two dimensions. The sample code below will perform the t-SNE method on a subset of the vocabulary. You can start with this code and modify it to construct visualizations of the embeddings, or start with your own version of t-SNE.\n",
    "The code provided shows the relative positions of the words conservative, liberal, elephant, and donkey based on GloVe embeddings. For your local embeddings, you may use similar code to visualize the embeddings. \n",
    "\n",
    "Find at least three more examples that produce expected results and three examples that produce surprising results for each of the two language models. Include the plots in your write-up. Give reasons why you might see surprising behavior here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# this functions computes and displays the 2-d t-SNE maps for a subset of the embedding vectors\n",
    "# and displays them together with the points for a set of input words.\n",
    "\n",
    "def display_tsne_neighborhood(model, input_word, nsample=1000, size1=2, size2=10, offset=5):\n",
    "    \n",
    "    arr = np.empty((0,100), dtype='f')\n",
    "    word_label = input_word\n",
    "\n",
    "    # add the vector for each of the closest words to the array\n",
    "    for w in range(len(input_word)):\n",
    "        arr = np.append(arr, np.array([model[input_word[w]]]), axis=0)\n",
    "\n",
    "    voc = [w for w in model.vocab]\n",
    "    wrds = np.random.choice(range(len(voc)), size=nsample, replace=False)\n",
    "    for w in wrds:\n",
    "        wrd_vector = model[voc[w]]\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coord = Y[:, 0]\n",
    "    y_coord = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    size=2\n",
    "    plt.scatter(x_coord, y_coord, s=size1)\n",
    "    plt.scatter(x_coord[0:len(input_word)], y_coord[0:len(input_word)],s=size2)\n",
    "    \n",
    "    # label the input words\n",
    "    for w in range(len(input_word)):\n",
    "        plt.annotate(input_word[w], xy=(x_coord[w],y_coord[w]), \\\n",
    "                     xytext=(w*offset,w*offset), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "display_tsne_neighborhood(glove, input_word = ['conservative', 'liberal', 'donkey', 'elephant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem  3: Experiments with Musician Embeddings (15 points)\n",
    "\n",
    "\n",
    "In this problem, we will use a collection of playlists obtained from [last.fm](http://last.fm). We treat each playlist as a document, and each artist in the playlist as a word. By feeding this dataset to word2vec, we will be able to learn artist embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artist Embeddings\n",
    "\n",
    "The following experiments will be done with the playlist data file `playlists.txt`. Each line in this file is a playlist. The integers on each line are unique artist identifiers, indicating which artists were in each playlist. The artists are in `artists.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below constructs artist embeddings with word2vec. The artist names are mapped to id numbers in the playlists; the code maps them back to display the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = word2vec.LineSentence('https://raw.githubusercontent.com/YData123/sds265-fa22/master/assignments/assn4/playlists.txt')\n",
    "music_model = word2vec.Word2Vec(playlists, size=64, window=100, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_model.wv['299']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "\n",
    "artist = []\n",
    "file = urlopen('https://raw.githubusercontent.com/YData123/sds265-fa22/master/assignments/assn4/artists.txt')\n",
    "for line in file:\n",
    "    art = line.decode(\"utf-8\")\n",
    "    artist.append(art.strip())\n",
    "\n",
    "artist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name = {}\n",
    "name2id = {}\n",
    "for w in range(len(artist)):\n",
    "    id2name[\"%s\" % w] = artist[w]\n",
    "    name2id[artist[w]] = \"%s\" % w\n",
    "\n",
    "id2name[name2id['Elton John']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Similar artists\n",
    "\n",
    "Find the 5 closest artist embedding vectors to the artists \"The Beatles\", \"Lady Gaga\", and \"Nirvana\". Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this function\n",
    "def similar_artists(model, artist, n=5):\n",
    "    id = name2id[artist]\n",
    "    out = model.wv.most_similar(id, topn=n)\n",
    "\n",
    "    print(\"artists similar to '%s'\\n\" % artist)\n",
    "    for i in range(n) :\n",
    "        name = id2name[out[i][0]]\n",
    "        print(\"\\t%s\" % name)\n",
    "        \n",
    "similar_artists(music_model, 'Aerosmith')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Visualize embeddings\n",
    "\n",
    "Use the t-SNE dimensionality reduction technique to visualize the artist embeddings. After running t-SNE on the artist embeddings, try visualizing  \"The Temptations\" and \"The Supremes\" together. Find a few more examples that you think are interesting and include the plots in your write-up. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# this functions computes and displays the 2-d t-SNE maps for a subset of the embedding vectors\n",
    "# and displays them together with the points for a set of input words.\n",
    "\n",
    "def display_tsne_artists(model, artists, nsample=1000, size1=2, size2=10, offset=5):\n",
    "    \n",
    "    arr = np.empty((0,64), dtype='f')\n",
    "\n",
    "    # add the vector for each of the closest words to the array\n",
    "    for a in range(len(artists)):\n",
    "        id = name2id[artists[a]]\n",
    "        arr = np.append(arr, np.array([model[id]]), axis=0)\n",
    "\n",
    "    voc = [w for w in model.wv.vocab]\n",
    "    ids = np.random.choice(range(len(voc)), size=nsample, replace=False)\n",
    "    for w in ids:\n",
    "        wrd_vector = model[voc[w]]\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coord = Y[:, 0]\n",
    "    y_coord = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    size=2\n",
    "    plt.scatter(x_coord, y_coord, s=size1)\n",
    "    plt.scatter(x_coord[0:len(artists)], y_coord[0:len(artists)],s=size2)\n",
    "    \n",
    "    # label the input words\n",
    "    for w in range(len(artists)):\n",
    "        plt.annotate(artists[w], xy=(x_coord[w],y_coord[w]), \\\n",
    "                     xytext=(w*offset,w*offset), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
