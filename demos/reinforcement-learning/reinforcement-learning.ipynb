{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HuysDQtl-te"
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    " In reinforcement learning, an agent interacts with the environment, experiencing a series of rewards based on its actions. The agent seeks to maximize its rewards by developing a strategy that learns to choose appropriate actions in each state. \n",
    " \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/demos/reinforcement-learning/rl_animation.gif\" width=300>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this notebook we demo the Q-learning algorithm, one of the fundamental algorithms of reinforcement learning.\n",
    "We illustrate Q-learning on the taxicab problem formulated by Tom Dietterich in the paper \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\", as developed in the code \n",
    "from [OpenAI gym](https://github.com/openai). Our presentation follows [this tutorial](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us_7J8RAl-tg"
   },
   "source": [
    "We'll need the OpenAI gym package. This can be installed as shown below. We'll display some simple graphics using `IPython.display`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLBwWqjpl-tg",
    "outputId": "29e11976-6c74-4f74-e39e-0a3e3d7f1f99"
   },
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install pygame\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzRybatAnXEO",
    "outputId": "989beba9-a946-4a33-ac17-72a9d6f6ff67"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import pygame\n",
    "_ = pygame.display.set_mode((640,480))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4dS3Iawl-th"
   },
   "source": [
    "The environment is a simple grid, with some barriers inserted to make things more interesting. A taxicab drives around the environment, picking up and delivering a passenger at four locations. A graphic of the environment is shown below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/demos/reinforcement-learning/taxi-shot.png\" width=200>\n",
    "\n",
    "The \"ascii art\" rendition of this environment appears as shown in the following output cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLuHQX0-l-th"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "print(env.render(mode='ansi'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbGfnIysl-ti"
   },
   "source": [
    "As described in the documentation, \n",
    "\"there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\"\n",
    "\n",
    "With 25 taxi positions, 4 possible locations of the passenger waiting, \n",
    "the case where the passenger is in the taxi, and 4 possible destination \n",
    "locations, the total number of states is $25\\times 5 \\times 4 = 500$.\n",
    "\n",
    "The passenger location are coded as integers in the following way:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "\n",
    "And the destinations are coded as:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "\n",
    "Finally, there are 6 possible actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "There is a default per-step reward of -1,\n",
    "and a reward of +20 for delivering the passenger. \n",
    "Carrying out a \"pickup\" or \"drop-off\" action illegally has\n",
    "a reward of -10.\n",
    "\n",
    "In the ascii art graphics, the following color schemes are used:\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n",
    "The state space is represented as a tuple in the following way:\n",
    "    \n",
    "    \n",
    "    state = (taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxnXJnJVl-tj"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for visualizing the sequence of states and actions\n",
    "\n",
    "def render(env, stat, action, reward):\n",
    "    return {'frame': env.render(mode='ansi'),\n",
    "            'state': stat,\n",
    "            'action': action,\n",
    "            'reward': reward}\n",
    "\n",
    "\n",
    "def print_frames(frames, delay=.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(delay)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgNR4qXGl-tj"
   },
   "source": [
    "#### Exploring randomly\n",
    "\n",
    "We first look at the obviously terrible strategy of selecting\n",
    "a random action at each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kuDyi7Fl-tj",
    "outputId": "1fbc3a00-5ea6-407b-9f38-7761cba57336"
   },
   "outputs": [],
   "source": [
    "env.s = 328  \n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "frames = [] \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample() # choose a random action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    frames.append(render(env, state, action, reward))\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GVSxdZbl-tk"
   },
   "source": [
    "As can be seen, the taxi wanders around randomly, banging into walls, picking up and dropping off the passenger at wrong places, and incurring large negative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOpI0j4bl-tk",
    "outputId": "1ff1019b-3ad8-4270-847f-372fa3e119d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_frames(frames[0:100], delay=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D5Rmz6Yl-tk"
   },
   "source": [
    "### Using Q-learning\n",
    "\n",
    "We'll now learn a strategy using the Q-learning algorithm. \n",
    "The Q-learning algorithm maintains a \"quality\" variable $Q(s,a)$ for taking action $a$ in state $s$. This is a measure of the cumulative\n",
    "rewards obtained by the algorithm when it takes action $a$ in state $s$. \n",
    "\n",
    "The quality should not be assessed \n",
    "purely based on the reward the action has in the current time step.\n",
    "Rather, we need to take into account the future rewards that the algorithm in driving toward the goal.\n",
    "\n",
    "As described in lecture, the quality function is updated\n",
    "as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(s,a) \\longleftarrow (1-\\alpha)\\,Q(s,a) + \\alpha \\left(\n",
    "\\mbox{reward}(s,a) + \\gamma \\max_{a'} Q(\\mbox{next}(s,a), a')\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Here $s$ is the current state. When action $a$ is taken, \n",
    "a reward $\\mbox{reward}(s,a)$ is given. Then, the algorithm \n",
    "moves to a new state, denoted $\\mbox{next}(s,a)$. For example, \n",
    "if the taxi is at location $(2,2)$ and takes the \"West\" action\n",
    "($a=3$), then there is a reward of -1, and the taxi moves \n",
    "to the new location $(2,3)$. If the cab is empty, it remains empty, and if it contains the passenger, the passenger remains.\n",
    "\n",
    "The cumulative future rewards of this action are given by the expression \n",
    "$\\max_{a'} Q(\\mbox{next}(s,a), a')$. These future rewards are discounted by a factor $\\gamma < 1$. This trades off short-term against long-term rewards. When $\\gamma$ is small, the short-term rewards are favored.\n",
    "\n",
    "Note that this update can be rewritten as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(s,a) \\longleftarrow \\,Q(s,a) + \\alpha \\left(\n",
    "\\mbox{reward}(s,a) + \\gamma \\max_{a'} Q(\\mbox{next}(s,a), a') - Q(s,a)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "This form suggests that it can be viewed as a gradient *ascent* algorithm, with $\\alpha$ as the step size. In fact, it can be shown that with descreasing step size the algorithm will converge, just as for stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJZgYNbUl-tk",
    "outputId": "a73f4088-644d-4c3a-e022-807a0411e032"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.8\n",
    "epsilon = 0.1\n",
    "episodes = 20000\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for _ in tqdm(np.arange(episodes)):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        greedy = True\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            greedy = False\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = old_value + alpha*(reward + gamma * next_max - old_value)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcEMELu8l-tk",
    "outputId": "a6acc168-59af-49cd-a9b2-80bdf8aac51e"
   },
   "outputs": [],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "epsilon = .001\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "           action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZwmAqT_l-tl",
    "outputId": "e34d5b74-f73a-4d4c-bc14-fa336a552ae2"
   },
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "       action = env.action_space.sample() # Explore action space\n",
    "    else:\n",
    "       action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append(render(env, state, action, reward))\n",
    "    epochs += 1\n",
    "    \n",
    "print_frames(frames, delay=.2)\n",
    "\n",
    "print(f\"\\nTimesteps taken: {epochs}\")\n",
    "print(f\"Penalties incurred: {penalties}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvJrW4Jpl-tl"
   },
   "source": [
    "Now replay the trip in 'slow motion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dLvXCW-l-tl",
    "outputId": "7123345c-71d8-4df4-c8f3-5354dbefee57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_frames(frames, delay=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOVsRVqpl-tl"
   },
   "source": [
    "### Visualizing the value function \n",
    "\n",
    "Now we compute the value function $v_*(s) = \\max_a Q(s,a)$ and visualize this during the taxi trip. We can see how this makes sense as a prediction of future reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "0v9eMN5Pl-tl",
    "outputId": "f54038db-e290-42df-8b64-c357e474ef92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = [np.max(q_table[frames[i]['state']]) for i in np.arange(len(frames))]\n",
    "\n",
    "for i in np.arange(len(frames)):\n",
    "    clear_output(wait=True)\n",
    "    print(frames[i]['frame']) \n",
    "    _ = plt.bar(x=np.arange(len(values)), height=values)\n",
    "    plt.axvline(x=i, c='gray', linestyle='dotted')\n",
    "    plt.show()\n",
    "    sleep(.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "_LhKemSql-tl",
    "outputId": "b4eb3ea4-72f8-4c47-8b2a-b420fad9f6c4"
   },
   "outputs": [],
   "source": [
    "value = np.zeros(25).reshape(5,5)\n",
    "for i in np.arange(500):\n",
    "    env.s = i\n",
    "    taxi_row, taxi_col, passenger_index, destination_index = env.decode(i)\n",
    "    if (passenger_index==4) and (destination_index==0):\n",
    "        v = np.max(q_table[i])\n",
    "        value[taxi_row, taxi_col] = v\n",
    "        #print(render(env, -1, -1, -1)['frame'])\n",
    "        #print('row=%d col=%d value=%f' % (taxi_row, taxi_col, v))\n",
    "        \n",
    "plt.imshow(value, cmap='jet')\n",
    "plt.colorbar()\n",
    "#plt.savefig('taxi1.jpg', bbox_inches='tight')\n",
    "np.round(value,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkKkUsaGl-tl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
